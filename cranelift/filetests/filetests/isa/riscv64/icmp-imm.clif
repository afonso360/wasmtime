test compile precise-output
target riscv64


function %icmp_eq_i8_imm(i8) -> i8 {
block0(v0: i8):
    v1 = iconst.i8 42
    v2 = icmp eq v0, v1
    return v2
}

; VCode:
; block0:
;   andi t2,a0,255
;   li a1,42
;   andi a3,a1,255
;   xor a5,t2,a3
;   seqz a0,a5
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   andi t2, a0, 0xff
;   addi a1, zero, 0x2a
;   andi a3, a1, 0xff
;   xor a5, t2, a3
;   seqz a0, a5
;   ret

function %icmp_slt_i64_imm32(i64) -> i8 {
block0(v0: i64):
  v1 = iconst.i64 32768
  v2 = icmp.i64 slt v0, v1
  return v2
}

; VCode:
; block0:
;   lui t2,8
;   slt a0,a0,t2
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   lui t2, 8
;   slt a0, a0, t2
;   ret

function %icmp_slt_i32_imm16(i32) -> i8 {
block0(v0: i32):
  v1 = iconst.i32 1
  v2 = icmp.i32 slt v0, v1
  return v2
}

; VCode:
; block0:
;   sext.w t2,a0
;   slti a0,t2,1
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   sext.w t2, a0
;   slti a0, t2, 1
;   ret

function %icmp_slt_i32_imm(i32) -> i8 {
block0(v0: i32):
  v1 = iconst.i32 32768
  v2 = icmp.i32 slt v0, v1
  return v2
}

; VCode:
; block0:
;   lui t2,8
;   sext.w a1,a0
;   sext.w a3,t2
;   slt a0,a1,a3
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   lui t2, 8
;   sext.w a1, a0
;   sext.w a3, t2
;   slt a0, a1, a3
;   ret

function %icmp_slt_i16_imm(i16) -> i8 {
block0(v0: i16):
    v1 = iconst.i16 42
    v2 = icmp eq v0, v1
    return v2
}

; VCode:
; block0:
;   slli t2,a0,48
;   srli a1,t2,48
;   li a3,42
;   slli a5,a3,48
;   srli a7,a5,48
;   xor t4,a1,a7
;   seqz a0,t4
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   slli t2, a0, 0x30
;   srli a1, t2, 0x30
;   addi a3, zero, 0x2a
;   slli a5, a3, 0x30
;   srli a7, a5, 0x30
;   xor t4, a1, a7
;   seqz a0, t4
;   ret

function %icmp_eq_i32_imm(i32) -> i8 {
block0(v0: i32):
    v1 = iconst.i32 42
    v2 = icmp eq v0, v1
    return v2
}

; VCode:
; block0:
;   sext.w t2,a0
;   li a1,42
;   sext.w a3,a1
;   xor a5,t2,a3
;   seqz a0,a5
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   sext.w t2, a0
;   addi a1, zero, 0x2a
;   sext.w a3, a1
;   xor a5, t2, a3
;   seqz a0, a5
;   ret

function %icmp_eq_i64_imm(i64) -> i8 {
block0(v0: i64):
    v1 = iconst.i64 42
    v2 = icmp eq v0, v1
    return v2
}

; VCode:
; block0:
;   li t2,42
;   xor a1,a0,t2
;   seqz a0,a1
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   addi t2, zero, 0x2a
;   xor a1, a0, t2
;   seqz a0, a1
;   ret

function %icmp_ne_i8_imm(i8) -> i8 {
block0(v0: i8):
    v1 = iconst.i8 42
    v2 = icmp ne v0, v1
    return v2
}

; VCode:
; block0:
;   andi t2,a0,255
;   li a1,42
;   andi a3,a1,255
;   xor a5,t2,a3
;   snez a0,a5
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   andi t2, a0, 0xff
;   addi a1, zero, 0x2a
;   andi a3, a1, 0xff
;   xor a5, t2, a3
;   snez a0, a5
;   ret

function %icmp_ne_i16_imm(i16) -> i8 {
block0(v0: i16):
    v1 = iconst.i16 42
    v2 = icmp ne v0, v1
    return v2
}

; VCode:
; block0:
;   slli t2,a0,48
;   srli a1,t2,48
;   li a3,42
;   slli a5,a3,48
;   srli a7,a5,48
;   xor t4,a1,a7
;   snez a0,t4
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   slli t2, a0, 0x30
;   srli a1, t2, 0x30
;   addi a3, zero, 0x2a
;   slli a5, a3, 0x30
;   srli a7, a5, 0x30
;   xor t4, a1, a7
;   snez a0, t4
;   ret

function %icmp_ne_i32_imm(i32) -> i8 {
block0(v0: i32):
    v1 = iconst.i32 42
    v2 = icmp ne v0, v1
    return v2
}

; VCode:
; block0:
;   sext.w t2,a0
;   li a1,42
;   sext.w a3,a1
;   xor a5,t2,a3
;   snez a0,a5
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   sext.w t2, a0
;   addi a1, zero, 0x2a
;   sext.w a3, a1
;   xor a5, t2, a3
;   snez a0, a5
;   ret

function %icmp_ne_i64_imm(i64) -> i8 {
block0(v0: i64):
    v1 = iconst.i64 42
    v2 = icmp ne v0, v1
    return v2
}

; VCode:
; block0:
;   li t2,42
;   xor a1,a0,t2
;   snez a0,a1
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   addi t2, zero, 0x2a
;   xor a1, a0, t2
;   snez a0, a1
;   ret

function %icmp_sge_i8_imm(i8) -> i8 {
block0(v0: i8):
    v1 = iconst.i8 42
    v2 = icmp sge v0, v1
    return v2
}

; VCode:
; block0:
;   slli t2,a0,56
;   srai a1,t2,56
;   li a3,42
;   slli a5,a3,56
;   srai a7,a5,56
;   slt t4,a1,a7
;   xori a0,t4,1
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   slli t2, a0, 0x38
;   srai a1, t2, 0x38
;   addi a3, zero, 0x2a
;   slli a5, a3, 0x38
;   srai a7, a5, 0x38
;   slt t4, a1, a7
;   xori a0, t4, 1
;   ret

function %icmp_sge_i16_imm(i16) -> i8 {
block0(v0: i16):
    v1 = iconst.i16 42
    v2 = icmp sge v0, v1
    return v2
}

; VCode:
; block0:
;   slli t2,a0,48
;   srai a1,t2,48
;   li a3,42
;   slli a5,a3,48
;   srai a7,a5,48
;   slt t4,a1,a7
;   xori a0,t4,1
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   slli t2, a0, 0x30
;   srai a1, t2, 0x30
;   addi a3, zero, 0x2a
;   slli a5, a3, 0x30
;   srai a7, a5, 0x30
;   slt t4, a1, a7
;   xori a0, t4, 1
;   ret

function %icmp_slt_i8_imm(i8) -> i8 {
block0(v0: i8):
    v1 = iconst.i8 42
    v2 = icmp slt v0, v1
    return v2
}

; VCode:
; block0:
;   slli t2,a0,56
;   srai a1,t2,56
;   slti a0,a1,42
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   slli t2, a0, 0x38
;   srai a1, t2, 0x38
;   slti a0, a1, 0x2a
;   ret

function %icmp_slt_i16_imm(i16) -> i8 {
block0(v0: i16):
    v1 = iconst.i16 42
    v2 = icmp slt v0, v1
    return v2
}

; VCode:
; block0:
;   slli t2,a0,48
;   srai a1,t2,48
;   slti a0,a1,42
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   slli t2, a0, 0x30
;   srai a1, t2, 0x30
;   slti a0, a1, 0x2a
;   ret

function %icmp_slt_i32_imm(i32) -> i8 {
block0(v0: i32):
    v1 = iconst.i32 42
    v2 = icmp slt v0, v1
    return v2
}

; VCode:
; block0:
;   sext.w t2,a0
;   slti a0,t2,42
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   sext.w t2, a0
;   slti a0, t2, 0x2a
;   ret

function %icmp_slt_i64_imm(i64) -> i8 {
block0(v0: i64):
    v1 = iconst.i64 42
    v2 = icmp slt v0, v1
    return v2
}

; VCode:
; block0:
;   slti a0,a0,42
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   slti a0, a0, 0x2a
;   ret

function %icmp_uge_i8_imm(i8) -> i8 {
block0(v0: i8):
    v1 = iconst.i8 42
    v2 = icmp uge v0, v1
    return v2
}

; VCode:
; block0:
;   andi t2,a0,255
;   li a1,42
;   andi a3,a1,255
;   sltu a5,t2,a3
;   xori a0,a5,1
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   andi t2, a0, 0xff
;   addi a1, zero, 0x2a
;   andi a3, a1, 0xff
;   sltu a5, t2, a3
;   xori a0, a5, 1
;   ret

function %icmp_uge_i16_imm(i16) -> i8 {
block0(v0: i16):
    v1 = iconst.i16 42
    v2 = icmp uge v0, v1
    return v2
}

; VCode:
; block0:
;   slli t2,a0,48
;   srli a1,t2,48
;   li a3,42
;   slli a5,a3,48
;   srli a7,a5,48
;   sltu t4,a1,a7
;   xori a0,t4,1
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   slli t2, a0, 0x30
;   srli a1, t2, 0x30
;   addi a3, zero, 0x2a
;   slli a5, a3, 0x30
;   srli a7, a5, 0x30
;   sltu t4, a1, a7
;   xori a0, t4, 1
;   ret

function %icmp_uge_i32_imm(i32) -> i8 {
block0(v0: i32):
    v1 = iconst.i32 42
    v2 = icmp uge v0, v1
    return v2
}

; VCode:
; block0:
;   slli t2,a0,32
;   srli a1,t2,32
;   li a3,42
;   slli a5,a3,32
;   srli a7,a5,32
;   sltu t4,a1,a7
;   xori a0,t4,1
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   slli t2, a0, 0x20
;   srli a1, t2, 0x20
;   addi a3, zero, 0x2a
;   slli a5, a3, 0x20
;   srli a7, a5, 0x20
;   sltu t4, a1, a7
;   xori a0, t4, 1
;   ret

function %icmp_ult_i16_imm(i16) -> i8 {
block0(v0: i16):
    v1 = iconst.i16 42
    v2 = icmp ult v0, v1
    return v2
}

; VCode:
; block0:
;   slli t2,a0,48
;   srli a1,t2,48
;   sltiu a0,a1,42
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   slli t2, a0, 0x30
;   srli a1, t2, 0x30
;   sltiu a0, a1, 0x2a
;   ret

function %icmp_ult_i32_imm(i32) -> i8 {
block0(v0: i32):
    v1 = iconst.i32 42
    v2 = icmp ult v0, v1
    return v2
}

; VCode:
; block0:
;   slli t2,a0,32
;   srli a1,t2,32
;   sltiu a0,a1,42
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   slli t2, a0, 0x20
;   srli a1, t2, 0x20
;   sltiu a0, a1, 0x2a
;   ret

function %icmp_slt_i8_imm_0(i8) -> i8 {
block0(v0: i8):
    v1 = iconst.i8 0
    v2 = icmp slt v0, v1
    return v2
}

; VCode:
; block0:
;   slli t2,a0,56
;   srai a1,t2,56
;   sltz a0,a1
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   slli t2, a0, 0x38
;   srai a1, t2, 0x38
;   sltz a0, a1
;   ret

function %icmp_slt_i16_imm_0(i16) -> i8 {
block0(v0: i16):
    v1 = iconst.i16 0
    v2 = icmp slt v0, v1
    return v2
}

; VCode:
; block0:
;   slli t2,a0,48
;   srai a1,t2,48
;   sltz a0,a1
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   slli t2, a0, 0x30
;   srai a1, t2, 0x30
;   sltz a0, a1
;   ret

function %icmp_slt_i32_imm_0(i32) -> i8 {
block0(v0: i32):
    v1 = iconst.i32 0
    v2 = icmp slt v0, v1
    return v2
}

; VCode:
; block0:
;   sext.w t2,a0
;   sltz a0,t2
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   sext.w t2, a0
;   sltz a0, t2
;   ret

function %icmp_slt_i64_imm_0(i64) -> i8 {
block0(v0: i64):
    v1 = iconst.i64 0
    v2 = icmp slt v0, v1
    return v2
}

; VCode:
; block0:
;   sltz a0,a0
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   sltz a0, a0
;   ret



function %icmp_sgt_i8_imm_0(i8) -> i8 {
block0(v0: i8):
    v1 = iconst.i8 0
    v2 = icmp sgt v0, v1
    return v2
}

; VCode:
; block0:
;   slli t2,a0,56
;   srai a1,t2,56
;   sgtz a0,a1
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   slli t2, a0, 0x38
;   srai a1, t2, 0x38
;   sgtz a0, a1
;   ret

function %icmp_sgt_i16_imm_0(i16) -> i8 {
block0(v0: i16):
    v1 = iconst.i16 0
    v2 = icmp sgt v0, v1
    return v2
}

; VCode:
; block0:
;   slli t2,a0,48
;   srai a1,t2,48
;   sgtz a0,a1
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   slli t2, a0, 0x30
;   srai a1, t2, 0x30
;   sgtz a0, a1
;   ret

function %icmp_sgt_i32_imm_0(i32) -> i8 {
block0(v0: i32):
    v1 = iconst.i32 0
    v2 = icmp sgt v0, v1
    return v2
}

; VCode:
; block0:
;   sext.w t2,a0
;   sgtz a0,t2
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   sext.w t2, a0
;   sgtz a0, t2
;   ret

function %icmp_sgt_i64_imm_0(i64) -> i8 {
block0(v0: i64):
    v1 = iconst.i64 0
    v2 = icmp sgt v0, v1
    return v2
}

; VCode:
; block0:
;   sgtz a0,a0
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   sgtz a0, a0
;   ret

