test compile precise-output
set unwind_info=false
target riscv64 has_v

function %swidenhigh_i8x16(i8x16) -> i16x8 {
block0(v0: i8x16):
    v1 = swiden_high v0
    return v1
}

; VCode:
;   add sp,-16
;   sd ra,8(sp)
;   sd fp,0(sp)
;   mv fp,sp
; block0:
;   vle8.v v9,16(fp) #avl=16, #vtype=(e8, m1, ta, ma)
;   vslidedown.vi v12,v9,8 #avl=16, #vtype=(e8, m1, ta, ma)
;   vsext.vf2 v14,v12 #avl=8, #vtype=(e16, m1, ta, ma)
;   vse8.v v14,0(a0) #avl=16, #vtype=(e8, m1, ta, ma)
;   ld ra,8(sp)
;   ld fp,0(sp)
;   add sp,+16
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   c.addi16sp sp, -0x10
;   sd ra, 8(sp)
;   sd s0, 0(sp)
;   c.mv s0, sp
; block1: ; offset 0xc
;   .byte 0x57, 0x70
;   c.sw a0, 0x18(s0)
;   addi t6, s0, 0x10
;   .byte 0x87, 0x84
;   .byte 0x0f, 0x02
;   .byte 0x57, 0x36
;   c.fld fa3, 0x38(a3)
;   .byte 0x57, 0x70
;   c.sw s1, 0x18(s1)
;   .byte 0x57, 0xa7
;   fmadd.s fs5, fa4, ft5, fa4, rmm
;   c.sw a0, 0x18(s0)
;   .byte 0x27, 0x07
;   c.addi tp, 1
;   ld ra, 8(sp)
;   ld s0, 0(sp)
;   c.addi16sp sp, 0x10
;   ret

function %swidenhigh_i16x8(i16x8) -> i32x4 {
block0(v0: i16x8):
    v1 = swiden_high v0
    return v1
}

; VCode:
;   add sp,-16
;   sd ra,8(sp)
;   sd fp,0(sp)
;   mv fp,sp
; block0:
;   vle8.v v9,16(fp) #avl=16, #vtype=(e8, m1, ta, ma)
;   vslidedown.vi v12,v9,4 #avl=8, #vtype=(e16, m1, ta, ma)
;   vsext.vf2 v14,v12 #avl=4, #vtype=(e32, m1, ta, ma)
;   vse8.v v14,0(a0) #avl=16, #vtype=(e8, m1, ta, ma)
;   ld ra,8(sp)
;   ld fp,0(sp)
;   add sp,+16
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   c.addi16sp sp, -0x10
;   sd ra, 8(sp)
;   sd s0, 0(sp)
;   c.mv s0, sp
; block1: ; offset 0xc
;   .byte 0x57, 0x70
;   c.sw a0, 0x18(s0)
;   addi t6, s0, 0x10
;   .byte 0x87, 0x84
;   .byte 0x0f, 0x02
;   .byte 0x57, 0x70
;   c.sw s1, 0x18(s1)
;   .byte 0x57, 0x36
;   c.fldsp ft9, 0x120(sp)
;   .byte 0x57, 0x70
;   c.swsp zero, 0x98(sp)
;   .byte 0x57, 0xa7
;   fmadd.s fs5, fa4, ft5, fa4, rmm
;   c.sw a0, 0x18(s0)
;   .byte 0x27, 0x07
;   c.addi tp, 1
;   ld ra, 8(sp)
;   ld s0, 0(sp)
;   c.addi16sp sp, 0x10
;   ret

function %swidenhigh_i32x4(i32x4) -> i64x2 {
block0(v0: i32x4):
    v1 = swiden_high v0
    return v1
}

; VCode:
;   add sp,-16
;   sd ra,8(sp)
;   sd fp,0(sp)
;   mv fp,sp
; block0:
;   vle8.v v9,16(fp) #avl=16, #vtype=(e8, m1, ta, ma)
;   vslidedown.vi v12,v9,2 #avl=4, #vtype=(e32, m1, ta, ma)
;   vsext.vf2 v14,v12 #avl=2, #vtype=(e64, m1, ta, ma)
;   vse8.v v14,0(a0) #avl=16, #vtype=(e8, m1, ta, ma)
;   ld ra,8(sp)
;   ld fp,0(sp)
;   add sp,+16
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   c.addi16sp sp, -0x10
;   sd ra, 8(sp)
;   sd s0, 0(sp)
;   c.mv s0, sp
; block1: ; offset 0xc
;   .byte 0x57, 0x70
;   c.sw a0, 0x18(s0)
;   addi t6, s0, 0x10
;   .byte 0x87, 0x84
;   .byte 0x0f, 0x02
;   .byte 0x57, 0x70
;   c.swsp zero, 0x98(sp)
;   .byte 0x57, 0x36
;   c.addiw t4, -0x1c
;   .byte 0x57, 0x70
;   c.beqz a1, 0x18
;   .byte 0x57, 0xa7
;   fmadd.s fs5, fa4, ft5, fa4, rmm
;   c.sw a0, 0x18(s0)
;   .byte 0x27, 0x07
;   c.addi tp, 1
;   ld ra, 8(sp)
;   ld s0, 0(sp)
;   c.addi16sp sp, 0x10
;   ret

function %swidenhigh_twice_i8x16(i8x16) -> i32x4 {
block0(v0: i8x16):
    v1 = swiden_high v0
    v2 = swiden_high v1
    return v2
}

; VCode:
;   add sp,-16
;   sd ra,8(sp)
;   sd fp,0(sp)
;   mv fp,sp
; block0:
;   vle8.v v9,16(fp) #avl=16, #vtype=(e8, m1, ta, ma)
;   vslidedown.vi v12,v9,12 #avl=16, #vtype=(e8, m1, ta, ma)
;   vsext.vf4 v14,v12 #avl=4, #vtype=(e32, m1, ta, ma)
;   vse8.v v14,0(a0) #avl=16, #vtype=(e8, m1, ta, ma)
;   ld ra,8(sp)
;   ld fp,0(sp)
;   add sp,+16
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   c.addi16sp sp, -0x10
;   sd ra, 8(sp)
;   sd s0, 0(sp)
;   c.mv s0, sp
; block1: ; offset 0xc
;   .byte 0x57, 0x70
;   c.sw a0, 0x18(s0)
;   addi t6, s0, 0x10
;   .byte 0x87, 0x84
;   .byte 0x0f, 0x02
;   .byte 0x57, 0x36
;   c.fldsp ft9, 0x160(sp)
;   .byte 0x57, 0x70
;   c.swsp zero, 0x98(sp)
;   .byte 0x57, 0xa7
;   c.lwsp s5, 0x10(sp)
;   .byte 0x57, 0x70
;   c.sw a0, 0x18(s0)
;   .byte 0x27, 0x07
;   c.addi tp, 1
;   ld ra, 8(sp)
;   ld s0, 0(sp)
;   c.addi16sp sp, 0x10
;   ret

function %swidenhigh_twice_i16x8(i16x8) -> i64x2 {
block0(v0: i16x8):
    v1 = swiden_high v0
    v2 = swiden_high v1
    return v2
}

; VCode:
;   add sp,-16
;   sd ra,8(sp)
;   sd fp,0(sp)
;   mv fp,sp
; block0:
;   vle8.v v9,16(fp) #avl=16, #vtype=(e8, m1, ta, ma)
;   vslidedown.vi v12,v9,6 #avl=8, #vtype=(e16, m1, ta, ma)
;   vsext.vf4 v14,v12 #avl=2, #vtype=(e64, m1, ta, ma)
;   vse8.v v14,0(a0) #avl=16, #vtype=(e8, m1, ta, ma)
;   ld ra,8(sp)
;   ld fp,0(sp)
;   add sp,+16
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   c.addi16sp sp, -0x10
;   sd ra, 8(sp)
;   sd s0, 0(sp)
;   c.mv s0, sp
; block1: ; offset 0xc
;   .byte 0x57, 0x70
;   c.sw a0, 0x18(s0)
;   addi t6, s0, 0x10
;   .byte 0x87, 0x84
;   .byte 0x0f, 0x02
;   .byte 0x57, 0x70
;   c.sw s1, 0x18(s1)
;   .byte 0x57, 0x36
;   sltiu t4, a4, 0x705
;   c.beqz a1, 0x18
;   .byte 0x57, 0xa7
;   c.lwsp s5, 0x10(sp)
;   .byte 0x57, 0x70
;   c.sw a0, 0x18(s0)
;   .byte 0x27, 0x07
;   c.addi tp, 1
;   ld ra, 8(sp)
;   ld s0, 0(sp)
;   c.addi16sp sp, 0x10
;   ret

function %swidenhigh_triple_i8x16(i8x16) -> i64x2 {
block0(v0: i8x16):
    v1 = swiden_high v0
    v2 = swiden_high v1
    v3 = swiden_high v2
    return v3
}

; VCode:
;   add sp,-16
;   sd ra,8(sp)
;   sd fp,0(sp)
;   mv fp,sp
; block0:
;   vle8.v v9,16(fp) #avl=16, #vtype=(e8, m1, ta, ma)
;   vslidedown.vi v12,v9,14 #avl=16, #vtype=(e8, m1, ta, ma)
;   vsext.vf8 v14,v12 #avl=2, #vtype=(e64, m1, ta, ma)
;   vse8.v v14,0(a0) #avl=16, #vtype=(e8, m1, ta, ma)
;   ld ra,8(sp)
;   ld fp,0(sp)
;   add sp,+16
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   c.addi16sp sp, -0x10
;   sd ra, 8(sp)
;   sd s0, 0(sp)
;   c.mv s0, sp
; block1: ; offset 0xc
;   .byte 0x57, 0x70
;   c.sw a0, 0x18(s0)
;   addi t6, s0, 0x10
;   .byte 0x87, 0x84
;   .byte 0x0f, 0x02
;   .byte 0x57, 0x36
;   auipc t4, 0x70573
;   c.beqz a1, 0x18
;   .byte 0x57, 0xa7
;   c.li s5, 0x10
;   .byte 0x57, 0x70
;   c.sw a0, 0x18(s0)
;   .byte 0x27, 0x07
;   c.addi tp, 1
;   ld ra, 8(sp)
;   ld s0, 0(sp)
;   c.addi16sp sp, 0x10
;   ret

